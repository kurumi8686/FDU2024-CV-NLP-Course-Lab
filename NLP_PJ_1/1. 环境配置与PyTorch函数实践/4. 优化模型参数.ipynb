{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化模型参数\n",
    "\n",
    "现在我们有了一个模型和数据，是时候通过优化其参数来训练、验证和测试我们的模型了。训练模型是一个迭代的过程；在每次迭代中，模型对输出进行预测，计算其预测的误差（*损失*），收集误差相对于其参数的导数，然后使用梯度下降**优化**这些参数。\n",
    "\n",
    "## 前置代码\n",
    "我们先加载前几节关于Dataset, DataLoader和构建模型的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:53:55.323820Z",
     "start_time": "2023-09-01T11:53:55.223694Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, file_path, split):\n",
    "        file_path = f\"{file_path}/{split}.jsonl\"\n",
    "        self.datas = pd.read_json(file_path, lines=True).to_dict(orient='records')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx]\n",
    "\n",
    "max_length = 128  # 最大文本长度\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    # 提取每个样本中的文本和标签信息\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    labels = [sample['label'] for sample in batch]\n",
    "\n",
    "    # 截断或填充文本以确保不超过最大长度\n",
    "    texts_list = []\n",
    "    for text in texts:\n",
    "        encoded_text = tokenizer.encode(text, truncation=True, max_length=max_length, padding='max_length')\n",
    "        texts_list.append(encoded_text)\n",
    "\n",
    "    texts_tensor = [torch.Tensor(text) for text in texts_list]\n",
    "\n",
    "    # 将文本序列填充为相同长度\n",
    "    texts_tensor = pad_sequence(texts_tensor, batch_first=True, padding_value=0)\n",
    "\n",
    "    # 将标签转换为张量\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    return {'text': texts_tensor, 'label': labels_tensor}\n",
    "\n",
    "training_data = SST2Dataset(\".\", \"train\")\n",
    "test_data = SST2Dataset(\".\", \"test\")\n",
    "\n",
    "# Load bert_tokenizer locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True, collate_fn=my_collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True, collate_fn=my_collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(max_length, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数\n",
    "\n",
    "超参数是可调参数，允许我们控制模型优化过程。\n",
    "不同的超参数值可以影响模型的训练和收敛速度\n",
    "\n",
    "我们为训练定义了以下超参数：\n",
    " - **Epoch的数量** - 迭代数据集的次数\n",
    " - **批量大小** - 在更新参数之前通过网络传播的数据样本数量\n",
    " - **学习率** - 在每个批次/迭代次数中更新模型参数的程度。较小的值会导致较慢的学习速度，而较大的值可能会导致训练过程中出现不可预测的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:53:56.554744Z",
     "start_time": "2023-09-01T11:53:56.550418Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化循环\n",
    "\n",
    "一旦设置了超参数，就可以使用优化循环来训练和优化模型。优化循环的每个迭代称为一个**epoch**。\n",
    "\n",
    "每个epoch包括两个主要部分：\n",
    " - **训练循环** - 遍历训练数据集并尝试收敛到最佳参数。\n",
    " - **验证/测试循环** - 遍历测试数据集以检查模型性能是否在改善。\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "当面对一些训练数据时，我们未经训练的网络很可能不会给出正确的答案。**损失函数**用于测量获得的结果与目标值之间的不相似程度，而我们希望在训练过程中将损失函数最小化。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实的数据标签值进行比较。\n",
    "\n",
    "常见的损失函数包括用于回归任务的[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)（均方误差）和用于分类任务的[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)（负对数似然）。[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)结合了``nn.LogSoftmax``和``nn.NLLLoss``。\n",
    "\n",
    "我们将模型的输出logits传递给``nn.CrossEntropyLoss``，它将标准化logits并计算预测误差。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:53:57.650192Z",
     "start_time": "2023-09-01T11:53:57.645275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "\n",
    "优化是调整模型参数以减小每个训练步骤中模型误差的过程。**优化算法**定义了如何执行此过程（在本例中，我们使用随机梯度下降）。\n",
    "所有优化逻辑都封装在``optimizer``对象中。在这里，我们使用了SGD优化器；此外，PyTorch还提供了许多[不同的优化器](https://pytorch.org/docs/stable/optim.html)，如ADAM和RMSProp，它们对于不同类型的模型和数据效果更好。\n",
    "\n",
    "我们通过传入需要训练的模型参数并输入学习率超参数来初始化优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:53:58.557663Z",
     "start_time": "2023-09-01T11:53:58.547695Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练循环内部，优化过程分为三个步骤：\n",
    " * 调用``optimizer.zero_grad()``来重置模型参数的梯度。梯度默认会累积；为了防止重复计算，我们在每次迭代中显式将它们归零。\n",
    " * 通过调用``loss.backward()``来反向传播预测损失。PyTorch会将损失相对于每个参数的梯度存储起来。\n",
    " * 一旦我们有了梯度，就调用``optimizer.step()``来通过在反向传播中收集的梯度来调整参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整实现\n",
    "我们定义了``train_loop``，它循环执行我们的优化代码，以及``test_loop``，它评估模型在测试数据上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:54:00.052820Z",
     "start_time": "2023-09-01T11:54:00.045592Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # 设置模型为训练模式 - 对于批量归一化和dropout层很重要\n",
    "    # 在这种情况下不是必需的，但为了最佳实践添加了这个步骤。\n",
    "    model.train()\n",
    "    for batch, inputs in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(inputs[\"text\"])\n",
    "        loss = loss_fn(pred, inputs[\"label\"])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(inputs[\"label\"])\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # 设置模型为评估模式 - 对于批量归一化和dropout层很重要\n",
    "    # 在这种情况下不是必需的，但为了最佳实践添加了这个步骤。\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # 使用torch.no_grad()评估模型可以确保在测试模式下不计算梯度\n",
    "    # 还有助于减少需要计算梯度的张量的不必要的梯度计算和内存使用。\n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            pred = model(inputs[\"text\"])\n",
    "            test_loss += loss_fn(pred, inputs[\"label\"]).item()\n",
    "            correct += (pred.argmax(1) == inputs[\"label\"]).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们初始化损失函数和优化器，并将它们传递给``train_loop``和``test_loop``。\n",
    "随时增加epoch的数值以跟踪模型性能的改进。\n",
    "\n",
    "（本教程只将流程跑通，因此虽然loss下降但效果并不一定会提升。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T11:54:11.332915Z",
     "start_time": "2023-09-01T11:54:01.862222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 79.145821  [   16/ 6920]\n",
      "loss: 8.649393  [ 1616/ 6920]\n",
      "loss: 2.784671  [ 3216/ 6920]\n",
      "loss: 1.026397  [ 4816/ 6920]\n",
      "loss: 0.612357  [ 6416/ 6920]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.763575 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.613927  [   16/ 6920]\n",
      "loss: 0.660455  [ 1616/ 6920]\n",
      "loss: 0.635091  [ 3216/ 6920]\n",
      "loss: 1.044611  [ 4816/ 6920]\n",
      "loss: 0.670879  [ 6416/ 6920]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 0.762069 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.707522  [   16/ 6920]\n",
      "loss: 0.591276  [ 1616/ 6920]\n",
      "loss: 0.855261  [ 3216/ 6920]\n",
      "loss: 0.649688  [ 4816/ 6920]\n",
      "loss: 0.727412  [ 6416/ 6920]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 0.717605 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.649030  [   16/ 6920]\n",
      "loss: 0.659693  [ 1616/ 6920]\n",
      "loss: 0.703008  [ 3216/ 6920]\n",
      "loss: 0.662853  [ 4816/ 6920]\n",
      "loss: 0.690132  [ 6416/ 6920]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.725939 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.657365  [   16/ 6920]\n",
      "loss: 0.898076  [ 1616/ 6920]\n",
      "loss: 0.690572  [ 3216/ 6920]\n",
      "loss: 0.683705  [ 4816/ 6920]\n",
      "loss: 0.709747  [ 6416/ 6920]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 0.735501 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
