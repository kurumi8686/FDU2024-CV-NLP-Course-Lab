{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想情况下，我们希望我们处理数据集的代码与模型训练代码解耦，以获得更好的可读性和模块化性。\n",
    "PyTorch提供了两个数据原语：``torch.utils.data.DataLoader``和``torch.utils.data.Dataset``，它们允许我们使用预加载的数据集以及自己的数据。\n",
    "``Dataset``存储样本及其对应的标签，而``DataLoader``在``Dataset``周围包装了一个可迭代对象，以便轻松访问样本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据数据文件创建自己的Dataset\n",
    "\n",
    "一个自定义的`Dataset`类必须实现三个函数：`__init__`、`__len__`和`__getitem__`。\n",
    "接下来我们尝试使用SST-2数据集来实现自定义的Dataset类。SST-2是斯坦福提出的一个电影评论情感分类数据集，我们仅使用其中的dev集来进行实验。\n",
    "让我们来看一下这个实现：dev.jsonl文件中一行存储了一条样例数据，其中\"text\",\"label\"和\"label_text\"分别表示评论、情感标签值和情感标签。\n",
    "\n",
    "接下来开始定义Dataset，其中file_path表示数据路径，split表示数据是训练集、验证集还是测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "    # `__init__`函数仅在实例化`Dataset`对象时运行一次。我们在这里加载所有数据。\n",
    "    def __init__(self, file_path, split):\n",
    "        file_path = f\"{file_path}/{split}.jsonl\"\n",
    "        self.datas = pd.read_json(file_path, lines=True).to_dict(orient='records')\n",
    "\n",
    "    # len() 函数返回数据集的样本数\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    # `__getitem__`函数从给定索引``idx``的数据集中加载并返回一个样本。\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用DataLoaders准备数据进行训练\n",
    "``Dataset``一次检索我们数据集的一个样本。在训练模型时，通常我们希望以\"小批量\"的方式传递样本，每个训练轮次重新打乱数据顺序以减少模型过拟合，并使用Python的``multiprocessing``来加速数据检索。\n",
    "\n",
    "``DataLoader``是一个可迭代对象，它在简单的API中为我们抽象了这个复杂性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数据批量化\n",
    "\n",
    "我们知道自然语言是一些离散化的符号，如何将离散化的符号转换成计算机能理解的数字，再进行批量化（Tensor化）也是数据处理过程中重要的一步。\n",
    "\n",
    "下面是一个基本的使用Bert Tokenizer进行文本批量化的演示代码。这里还不了解BERT和Tokenizer没关系，我们现在就当Tokenizer是一个文本转数字的工具即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 128  # 最大文本长度\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    # 提取每个样本中的文本和标签信息\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    labels = [sample['label'] for sample in batch]\n",
    "\n",
    "    # 截断或填充文本以确保不超过最大长度\n",
    "    texts_list = []\n",
    "    for text in texts:\n",
    "        encoded_text = tokenizer.encode(text, truncation=True, max_length=max_length, padding='max_length')\n",
    "        texts_list.append(encoded_text)\n",
    "\n",
    "    texts_tensor = [torch.Tensor(text) for text in texts_list]\n",
    "\n",
    "    # 将文本序列填充为相同长度\n",
    "    texts_tensor = pad_sequence(texts_tensor, batch_first=True, padding_value=0)\n",
    "\n",
    "    # 将标签转换为张量\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    return {'text': texts_tensor, 'label': labels_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们分别定义训练集和测试集的DataLoader："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data = SST2Dataset(\".\", \"train\")\n",
    "test_data = SST2Dataset(\".\", \"test\")\n",
    "\n",
    "# Load bert_tokenizer locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=8, shuffle=True, collate_fn=my_collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=True, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遍历DataLoader\n",
    "\n",
    "我们已经将数据集加载到``DataLoader``中，并可以根据需要遍历数据集。\n",
    "下面的每次迭代都返回一个批次的``text``和``label``（分别包含``batch_size=8``个文本输入和标签）。\n",
    "因为我们指定了``shuffle=True``，所以在遍历完所有批次之后，数据将被重新打乱（如果需要更细粒度的控制数据加载顺序，请查看[Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts batch shape: torch.Size([8, 128])\n",
      "Labels batch shape: torch.Size([8])\n",
      "Text: tensor([  101.,  1036.,  2054.,  1005.,  1055.,  1996.,  2845.,  2773.,  2005.,\n",
      "        10166.,   999.,  1029.,  1005.,   102.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = next(iter(train_dataloader))\n",
    "train_texts, train_labels = inputs[\"text\"], inputs[\"label\"]\n",
    "print(f\"Texts batch shape: {train_texts.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "text= train_texts[0,:]\n",
    "label = train_labels[0]\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [torch.utils.data API](https://pytorch.org/docs/stable/data.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
